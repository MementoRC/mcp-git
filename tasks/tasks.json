{
  "tasks": [
    {
      "id": 1,
      "title": "Create Comprehensive Test Suite for Current Functionality",
      "description": "Establish a baseline test suite that covers existing functionality to ensure no regressions during enhancement implementation.",
      "details": "1. Identify all current MCP message types handled by the server\n2. Create unit tests for each message type processing\n3. Set up integration tests with mock MCP clients\n4. Implement test cases for Git operations\n5. Add performance benchmarks to measure current latency\n6. Create test fixtures for common scenarios\n7. Implement GitHub Actions workflow tests\n\nCode structure:\n```python\n# src/tests/test_current_functionality.py\nimport pytest\nfrom mcp_server_git.server import MCPGitServer\n\n@pytest.fixture\ndef mcp_server():\n    # Setup test server instance\n    server = MCPGitServer()\n    yield server\n    # Teardown\n\ndef test_existing_notification_handling(mcp_server):\n    # Test current notification handling\n    pass\n\ndef test_git_operations(mcp_server):\n    # Test git operations still work\n    pass\n\n# Add performance benchmarks\n@pytest.mark.benchmark\ndef test_message_processing_performance(benchmark, mcp_server):\n    # Benchmark current message processing\n    pass\n```",
      "testStrategy": "1. Verify all existing message types are handled correctly\n2. Ensure 95% code coverage for current functionality\n3. Measure and document current performance metrics\n4. Test GitHub Actions integration\n5. Validate Git operations work as expected\n6. Document any existing bugs or limitations as known issues",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Implement CancelledNotification Pydantic Model",
      "description": "Create a Pydantic model for the notifications/cancelled message type that is currently causing validation errors.",
      "details": "1. Review MCP protocol specification for cancelled notification format\n2. Create a new Pydantic model in models/notifications.py\n3. Include all required and optional fields according to spec\n4. Add proper type hints and validation rules\n5. Implement sensible defaults for optional fields\n6. Add docstrings explaining the model purpose and usage\n\n```python\n# src/mcp_server_git/models/notifications.py\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Any, Dict, List\n\nclass CancelledNotification(BaseModel):\n    \"\"\"Model for MCP notifications/cancelled message type.\"\"\"\n    type: str = Field(\"notifications/cancelled\", const=True)\n    id: str\n    request_id: str\n    reason: Optional[str] = None\n    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)\n    \n    class Config:\n        extra = \"ignore\"  # Allow additional fields for forward compatibility\n```",
      "testStrategy": "1. Unit test the model with valid cancelled notification examples\n2. Test with missing optional fields to verify defaults work\n3. Test with extra fields to ensure they're ignored\n4. Test with malformed data to verify validation errors\n5. Verify compatibility with actual MCP client messages\n6. Test serialization/deserialization roundtrip",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Update ClientNotification Union Type",
      "description": "Modify the existing ClientNotification union type to include the new CancelledNotification model and ensure all notification types are supported.",
      "details": "1. Locate the current ClientNotification union definition\n2. Add CancelledNotification to the union\n3. Review MCP spec for any other missing notification types\n4. Add any other missing notification types\n5. Implement type discrimination logic based on 'type' field\n6. Update docstrings and type hints\n\n```python\n# src/mcp_server_git/models/notifications.py\nfrom typing import Union, Literal, Dict, Any\nfrom pydantic import BaseModel, Field, root_validator\n\n# Import existing notification models\nfrom .existing_models import ExistingNotification1, ExistingNotification2\n\n# Add the new CancelledNotification model\n\n# Update the union type\nClientNotification = Union[\n    ExistingNotification1,\n    ExistingNotification2,\n    CancelledNotification,\n    # Add any other missing notification types\n]\n\n# Helper function for parsing notifications\ndef parse_client_notification(data: Dict[str, Any]) -> ClientNotification:\n    \"\"\"Parse a client notification from raw data based on its type field.\"\"\"\n    notification_type = data.get(\"type\", \"\")\n    \n    if notification_type == \"notifications/cancelled\":\n        return CancelledNotification.parse_obj(data)\n    elif notification_type == \"existing_type_1\":\n        return ExistingNotification1.parse_obj(data)\n    # Handle other types\n    else:\n        # Log unknown type but don't crash\n        logger.warning(f\"Unknown notification type: {notification_type}\")\n        # Return a generic notification or best guess\n```",
      "testStrategy": "1. Test parsing each notification type\n2. Verify type discrimination works correctly\n3. Test with all known notification types from MCP spec\n4. Test with unknown notification types to ensure graceful handling\n5. Verify backward compatibility with existing code\n6. Test with real-world message examples",
      "priority": "high",
      "dependencies": [
        2
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement Flexible Validation with Fallback Handling",
      "description": "Create a robust validation system that can handle unexpected or malformed messages without crashing the server.",
      "details": "1. Create a new validation.py module for centralized validation logic\n2. Implement try-catch wrappers for Pydantic validation\n3. Add fallback validation that extracts critical fields\n4. Create logging for validation failures\n5. Implement a validation strategy pattern for different message types\n\n```python\n# src/mcp_server_git/models/validation.py\nimport logging\nfrom typing import Dict, Any, Optional, TypeVar, Type, Generic\nfrom pydantic import BaseModel, ValidationError\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T', bound=BaseModel)\n\nclass ValidationResult(Generic[T]):\n    \"\"\"Container for validation results with error handling.\"\"\"\n    def __init__(self, model: Optional[T] = None, error: Optional[Exception] = None,\n                 raw_data: Optional[Dict[str, Any]] = None):\n        self.model = model\n        self.error = error\n        self.raw_data = raw_data or {}\n        self.is_valid = model is not None and error is None\n    \n    @property\n    def message_type(self) -> str:\n        \"\"\"Extract message type even if validation failed.\"\"\"\n        if self.model:\n            return getattr(self.model, \"type\", \"unknown\")\n        return self.raw_data.get(\"type\", \"unknown\")\n\ndef validate_message(data: Dict[str, Any], model_class: Type[T]) -> ValidationResult[T]:\n    \"\"\"Validate a message against a model with error handling.\"\"\"\n    try:\n        model = model_class.parse_obj(data)\n        return ValidationResult(model=model, raw_data=data)\n    except ValidationError as e:\n        logger.warning(f\"Validation error for {model_class.__name__}: {e}\")\n        return ValidationResult(error=e, raw_data=data)\n\ndef safe_parse_notification(data: Dict[str, Any]) -> ValidationResult:\n    \"\"\"Parse a notification with fallback handling.\"\"\"\n    from .notifications import parse_client_notification\n    \n    try:\n        model = parse_client_notification(data)\n        return ValidationResult(model=model, raw_data=data)\n    except Exception as e:\n        logger.error(f\"Failed to parse notification: {e}\")\n        return ValidationResult(error=e, raw_data=data)\n```",
      "testStrategy": "1. Test validation with valid messages\n2. Test with invalid messages to verify error handling\n3. Test extraction of critical fields from invalid messages\n4. Verify logging of validation errors\n5. Test with edge cases (empty messages, missing fields)\n6. Benchmark validation performance",
      "priority": "high",
      "dependencies": [
        3
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Create Error Recovery Mechanism",
      "description": "Implement a robust error recovery system that allows the server to continue operation after encountering non-critical errors.",
      "details": "1. Create a new error_handling.py module\n2. Implement error classification (critical vs. non-critical)\n3. Add recovery strategies for different error types\n4. Create decorators for wrapping error-prone functions\n5. Implement retry logic for transient errors\n\n```python\n# src/mcp_server_git/error_handling.py\nimport logging\nimport functools\nfrom typing import Callable, TypeVar, Any, Optional, Dict, List\nfrom enum import Enum\n\nlogger = logging.getLogger(__name__)\n\nclass ErrorSeverity(Enum):\n    CRITICAL = \"critical\"  # Session must terminate\n    HIGH = \"high\"          # Operation must abort, session can continue\n    MEDIUM = \"medium\"      # Operation might recover\n    LOW = \"low\"            # Can safely ignore\n\nclass ErrorContext:\n    \"\"\"Context information about an error.\"\"\"\n    def __init__(self, error: Exception, severity: ErrorSeverity = ErrorSeverity.MEDIUM,\n                 operation: str = \"\", session_id: Optional[str] = None,\n                 recoverable: bool = True, metadata: Optional[Dict[str, Any]] = None):\n        self.error = error\n        self.severity = severity\n        self.operation = operation\n        self.session_id = session_id\n        self.recoverable = recoverable\n        self.metadata = metadata or {}\n        self.handled = False\n\nT = TypeVar('T')\n\ndef recoverable(max_retries: int = 3, backoff_factor: float = 1.0):\n    \"\"\"Decorator for functions that should recover from errors.\"\"\"\n    def decorator(func: Callable[..., T]) -> Callable[..., T]:\n        @functools.wraps(func)\n        async def wrapper(*args: Any, **kwargs: Any) -> T:\n            retries = 0\n            last_error = None\n            \n            while retries <= max_retries:\n                try:\n                    return await func(*args, **kwargs)\n                except Exception as e:\n                    last_error = e\n                    retries += 1\n                    if retries <= max_retries:\n                        wait_time = backoff_factor * (2 ** (retries - 1))\n                        logger.warning(f\"Error in {func.__name__}, retry {retries}/{max_retries} after {wait_time}s: {e}\")\n                        await asyncio.sleep(wait_time)\n                    else:\n                        logger.error(f\"Failed after {max_retries} retries: {e}\")\n            \n            # If we get here, all retries failed\n            raise last_error\n        \n        return wrapper\n    return decorator\n\nasync def handle_error(context: ErrorContext) -> bool:\n    \"\"\"Central error handler that determines recovery strategy.\"\"\"\n    logger.error(f\"{context.severity.value.upper()} error in {context.operation}: {context.error}\")\n    \n    # Record error in metrics\n    # TODO: Add metrics integration\n    \n    if context.severity == ErrorSeverity.CRITICAL:\n        logger.critical(\"Critical error, cannot continue session\")\n        return False  # Cannot recover\n    \n    if not context.recoverable:\n        logger.error(\"Non-recoverable error, but session can continue\")\n        context.handled = True\n        return True  # Session continues, but operation failed\n    \n    # Implement recovery strategies based on error type\n    # ...\n    \n    context.handled = True\n    return True  # Recovered successfully\n```",
      "testStrategy": "1. Test recovery from different error types\n2. Verify retry logic works with transient errors\n3. Test handling of critical vs. non-critical errors\n4. Verify session continues after recoverable errors\n5. Test logging and metrics recording\n6. Simulate various error scenarios to test recovery paths",
      "priority": "high",
      "dependencies": [
        4
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Circuit Breaker Pattern",
      "description": "Add a circuit breaker mechanism to prevent cascading failures when repeated errors occur.",
      "details": "1. Create a CircuitBreaker class in error_handling.py\n2. Implement the three states: Closed, Open, Half-Open\n3. Add configurable thresholds for tripping the circuit\n4. Create tracking for error rates and types\n5. Implement automatic recovery after cooling period\n\n```python\n# Add to src/mcp_server_git/error_handling.py\nfrom enum import Enum\nimport time\nfrom typing import Dict, List, Optional, Callable, Any\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"       # Normal operation, allowing requests\n    OPEN = \"open\"           # Failing fast, not allowing requests\n    HALF_OPEN = \"half_open\" # Testing if system has recovered\n\nclass CircuitBreaker:\n    \"\"\"Implements the circuit breaker pattern to prevent cascading failures.\"\"\"\n    \n    def __init__(self, name: str, failure_threshold: int = 5,\n                 recovery_timeout: float = 30.0, half_open_max_calls: int = 1):\n        self.name = name\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.half_open_max_calls = half_open_max_calls\n        \n        self.state = CircuitState.CLOSED\n        self.failure_count = 0\n        self.last_failure_time = 0.0\n        self.half_open_calls = 0\n    \n    def reset(self) -> None:\n        \"\"\"Reset the circuit breaker to closed state.\"\"\"\n        self.state = CircuitState.CLOSED\n        self.failure_count = 0\n        self.last_failure_time = 0.0\n        self.half_open_calls = 0\n    \n    def record_failure(self) -> None:\n        \"\"\"Record a failure and potentially trip the circuit.\"\"\"\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        \n        if self.state == CircuitState.CLOSED and self.failure_count >= self.failure_threshold:\n            logger.warning(f\"Circuit {self.name} tripped after {self.failure_count} failures\")\n            self.state = CircuitState.OPEN\n        elif self.state == CircuitState.HALF_OPEN:\n            logger.warning(f\"Circuit {self.name} reopened after test failure\")\n            self.state = CircuitState.OPEN\n    \n    def record_success(self) -> None:\n        \"\"\"Record a success and potentially reset the circuit.\"\"\"\n        if self.state == CircuitState.HALF_OPEN:\n            self.reset()\n            logger.info(f\"Circuit {self.name} closed after successful test\")\n    \n    def allow_request(self) -> bool:\n        \"\"\"Check if a request should be allowed based on circuit state.\"\"\"\n        if self.state == CircuitState.CLOSED:\n            return True\n        \n        if self.state == CircuitState.OPEN:\n            # Check if recovery timeout has elapsed\n            if time.time() - self.last_failure_time >= self.recovery_timeout:\n                logger.info(f\"Circuit {self.name} entering half-open state for testing\")\n                self.state = CircuitState.HALF_OPEN\n                self.half_open_calls = 0\n            else:\n                return False  # Still open, fail fast\n        \n        if self.state == CircuitState.HALF_OPEN:\n            if self.half_open_calls < self.half_open_max_calls:\n                self.half_open_calls += 1\n                return True\n            return False\n        \n        return True\n\n# Circuit breaker decorator\ndef with_circuit_breaker(circuit: CircuitBreaker):\n    \"\"\"Decorator to apply circuit breaker to a function.\"\"\"\n    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n        @functools.wraps(func)\n        async def wrapper(*args: Any, **kwargs: Any) -> Any:\n            if not circuit.allow_request():\n                raise CircuitOpenError(f\"Circuit {circuit.name} is open\")\n            \n            try:\n                result = await func(*args, **kwargs)\n                circuit.record_success()\n                return result\n            except Exception as e:\n                circuit.record_failure()\n                raise\n        \n        return wrapper\n    return decorator\n\nclass CircuitOpenError(Exception):\n    \"\"\"Error raised when a circuit is open and rejects a request.\"\"\"\n    pass\n```",
      "testStrategy": "1. Test circuit transitions between states\n2. Verify circuit trips after threshold failures\n3. Test automatic recovery after timeout\n4. Verify half-open state behavior\n5. Test with concurrent requests\n6. Verify metrics and logging of circuit state changes",
      "priority": "medium",
      "dependencies": [
        5
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Create Session Management Module",
      "description": "Implement a dedicated session management module to track session lifecycle and health.",
      "details": "1. Create a new session.py module\n2. Implement SessionManager class for tracking active sessions\n3. Add session health monitoring and metrics\n4. Create session lifecycle events (create, start, pause, resume, end)\n5. Implement session state persistence\n\n```python\n# src/mcp_server_git/session.py\nimport asyncio\nimport logging\nimport time\nfrom typing import Dict, List, Optional, Any, Set\nfrom enum import Enum\nimport uuid\n\nlogger = logging.getLogger(__name__)\n\nclass SessionState(Enum):\n    INITIALIZING = \"initializing\"\n    ACTIVE = \"active\"\n    PAUSED = \"paused\"\n    ERROR = \"error\"\n    CLOSING = \"closing\"\n    CLOSED = \"closed\"\n\nclass Session:\n    \"\"\"Represents a client session with the MCP server.\"\"\"\n    \n    def __init__(self, session_id: Optional[str] = None):\n        self.session_id = session_id or str(uuid.uuid4())\n        self.state = SessionState.INITIALIZING\n        self.created_at = time.time()\n        self.last_activity = time.time()\n        self.error_count = 0\n        self.message_count = 0\n        self.client_info: Dict[str, Any] = {}\n        self.metadata: Dict[str, Any] = {}\n        self.active_operations: Set[str] = set()\n    \n    def update_activity(self) -> None:\n        \"\"\"Update the last activity timestamp.\"\"\"\n        self.last_activity = time.time()\n    \n    def record_message(self) -> None:\n        \"\"\"Record a message processed in this session.\"\"\"\n        self.message_count += 1\n        self.update_activity()\n    \n    def record_error(self) -> None:\n        \"\"\"Record an error in this session.\"\"\"\n        self.error_count += 1\n    \n    def add_operation(self, operation_id: str) -> None:\n        \"\"\"Add an active operation to this session.\"\"\"\n        self.active_operations.add(operation_id)\n    \n    def remove_operation(self, operation_id: str) -> None:\n        \"\"\"Remove an operation from this session.\"\"\"\n        if operation_id in self.active_operations:\n            self.active_operations.remove(operation_id)\n    \n    @property\n    def idle_time(self) -> float:\n        \"\"\"Get the time since last activity in seconds.\"\"\"\n        return time.time() - self.last_activity\n    \n    @property\n    def session_duration(self) -> float:\n        \"\"\"Get the total session duration in seconds.\"\"\"\n        return time.time() - self.created_at\n    \n    @property\n    def is_active(self) -> bool:\n        \"\"\"Check if the session is in an active state.\"\"\"\n        return self.state == SessionState.ACTIVE\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert session to a dictionary for serialization.\"\"\"\n        return {\n            \"session_id\": self.session_id,\n            \"state\": self.state.value,\n            \"created_at\": self.created_at,\n            \"last_activity\": self.last_activity,\n            \"error_count\": self.error_count,\n            \"message_count\": self.message_count,\n            \"active_operations\": list(self.active_operations),\n            \"client_info\": self.client_info,\n            \"metadata\": self.metadata,\n        }\n\nclass SessionManager:\n    \"\"\"Manages all active sessions in the MCP server.\"\"\"\n    \n    def __init__(self):\n        self.sessions: Dict[str, Session] = {}\n        self.lock = asyncio.Lock()\n    \n    async def create_session(self, session_id: Optional[str] = None) -> Session:\n        \"\"\"Create a new session.\"\"\"\n        async with self.lock:\n            session = Session(session_id)\n            self.sessions[session.session_id] = session\n            logger.info(f\"Created session {session.session_id}\")\n            return session\n    \n    async def get_session(self, session_id: str) -> Optional[Session]:\n        \"\"\"Get a session by ID.\"\"\"\n        async with self.lock:\n            return self.sessions.get(session_id)\n    \n    async def close_session(self, session_id: str) -> None:\n        \"\"\"Close and remove a session.\"\"\"\n        async with self.lock:\n            if session_id in self.sessions:\n                session = self.sessions[session_id]\n                session.state = SessionState.CLOSED\n                logger.info(f\"Closed session {session_id} after {session.session_duration:.2f}s\")\n                del self.sessions[session_id]\n    \n    async def get_all_sessions(self) -> List[Session]:\n        \"\"\"Get all active sessions.\"\"\"\n        async with self.lock:\n            return list(self.sessions.values())\n    \n    async def cleanup_idle_sessions(self, max_idle_time: float = 3600.0) -> int:\n        \"\"\"Clean up sessions that have been idle for too long.\"\"\"\n        to_close = []\n        \n        async with self.lock:\n            for session_id, session in self.sessions.items():\n                if session.idle_time > max_idle_time:\n                    to_close.append(session_id)\n        \n        for session_id in to_close:\n            await self.close_session(session_id)\n        \n        return len(to_close)\n```",
      "testStrategy": "1. Test session creation and lifecycle\n2. Verify session state transitions\n3. Test concurrent session operations\n4. Verify idle session cleanup\n5. Test session metrics and monitoring\n6. Verify session persistence and recovery",
      "priority": "medium",
      "dependencies": [
        5
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement Heartbeat Mechanism",
      "description": "Add a heartbeat system to detect client disconnections and maintain session health awareness.",
      "details": "1. Add heartbeat protocol support to session management\n2. Implement configurable heartbeat intervals\n3. Create timeout detection for missed heartbeats\n4. Add automatic session cleanup for disconnected clients\n5. Implement heartbeat response handling\n\n```python\n# Add to src/mcp_server_git/session.py\nimport asyncio\nfrom datetime import datetime\n\nclass HeartbeatManager:\n    \"\"\"Manages heartbeats for session health monitoring.\"\"\"\n    \n    def __init__(self, session_manager: SessionManager, \n                 heartbeat_interval: float = 30.0,\n                 missed_heartbeat_threshold: int = 3):\n        self.session_manager = session_manager\n        self.heartbeat_interval = heartbeat_interval\n        self.missed_heartbeat_threshold = missed_heartbeat_threshold\n        self.session_heartbeats: Dict[str, datetime] = {}\n        self.running = False\n        self.task: Optional[asyncio.Task] = None\n    \n    async def start(self) -> None:\n        \"\"\"Start the heartbeat monitoring task.\"\"\"\n        if self.running:\n            return\n        \n        self.running = True\n        self.task = asyncio.create_task(self._heartbeat_loop())\n    \n    async def stop(self) -> None:\n        \"\"\"Stop the heartbeat monitoring task.\"\"\"\n        self.running = False\n        if self.task:\n            self.task.cancel()\n            try:\n                await self.task\n            except asyncio.CancelledError:\n                pass\n            self.task = None\n    \n    async def _heartbeat_loop(self) -> None:\n        \"\"\"Main heartbeat monitoring loop.\"\"\"\n        while self.running:\n            try:\n                await self._check_heartbeats()\n                await asyncio.sleep(self.heartbeat_interval)\n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                logger.error(f\"Error in heartbeat loop: {e}\")\n                await asyncio.sleep(5.0)  # Backoff on error\n    \n    async def _check_heartbeats(self) -> None:\n        \"\"\"Check all sessions for heartbeat status.\"\"\"\n        now = datetime.now()\n        sessions = await self.session_manager.get_all_sessions()\n        \n        for session in sessions:\n            # Skip sessions that aren't fully active yet\n            if session.state != SessionState.ACTIVE:\n                continue\n                \n            last_heartbeat = self.session_heartbeats.get(session.session_id)\n            if not last_heartbeat:\n                # New session, initialize heartbeat\n                self.session_heartbeats[session.session_id] = now\n                continue\n            \n            # Calculate time since last heartbeat\n            time_diff = (now - last_heartbeat).total_seconds()\n            missed_beats = int(time_diff / self.heartbeat_interval)\n            \n            if missed_beats >= self.missed_heartbeat_threshold:\n                logger.warning(f\"Session {session.session_id} missed {missed_beats} heartbeats, closing\")\n                await self.session_manager.close_session(session.session_id)\n                if session.session_id in self.session_heartbeats:\n                    del self.session_heartbeats[session.session_id]\n    \n    async def record_heartbeat(self, session_id: str) -> None:\n        \"\"\"Record a heartbeat from a client.\"\"\"\n        self.session_heartbeats[session_id] = datetime.now()\n        \n        # Update session activity\n        session = await self.session_manager.get_session(session_id)\n        if session:\n            session.update_activity()\n\n# Add heartbeat message handling to server.py\nasync def handle_heartbeat(self, message: Dict[str, Any], session_id: str) -> None:\n    \"\"\"Handle a heartbeat message from a client.\"\"\"\n    await self.heartbeat_manager.record_heartbeat(session_id)\n    \n    # Send heartbeat response if needed\n    response = {\n        \"type\": \"heartbeat\",\n        \"id\": str(uuid.uuid4()),\n        \"timestamp\": datetime.now().isoformat(),\n    }\n    await self.send_message(response, session_id)\n```",
      "testStrategy": "1. Test heartbeat message handling\n2. Verify missed heartbeat detection\n3. Test automatic session cleanup\n4. Verify heartbeat intervals are respected\n5. Test with simulated network interruptions\n6. Verify heartbeat metrics are recorded",
      "priority": "medium",
      "dependencies": [
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Update Server Message Handling for Cancellation",
      "description": "Modify the server's message handling to properly process cancellation notifications and maintain session stability.",
      "details": "1. Update the main message handling loop in server.py\n2. Add specific handling for cancellation notifications\n3. Implement operation cancellation logic\n4. Add error recovery for message processing\n5. Update session state based on message outcomes\n\n```python\n# src/mcp_server_git/server.py\nimport asyncio\nimport logging\nfrom typing import Dict, Any, Optional, List\n\nfrom .models.notifications import parse_client_notification, CancelledNotification\nfrom .models.validation import validate_message, ValidationResult\nfrom .error_handling import handle_error, ErrorContext, ErrorSeverity, recoverable\nfrom .session import SessionManager, Session\n\nlogger = logging.getLogger(__name__)\n\nclass MCPGitServer:\n    # ... existing initialization code ...\n    \n    @recoverable(max_retries=3)\n    async def process_message(self, raw_message: Dict[str, Any], session_id: str) -> None:\n        \"\"\"Process an incoming message with error handling.\"\"\"\n        session = await self.session_manager.get_session(session_id)\n        if not session:\n            logger.warning(f\"Message received for unknown session {session_id}\")\n            return\n        \n        session.record_message()\n        \n        try:\n            # Validate the message\n            validation_result = validate_message(raw_message, self.get_model_for_message_type(raw_message))\n            \n            if not validation_result.is_valid:\n                # Handle validation failure\n                logger.warning(f\"Invalid message format: {validation_result.error}\")\n                # Continue processing with best effort if possible\n            \n            # Process based on message type\n            message_type = validation_result.message_type\n            \n            if message_type == \"notifications/cancelled\":\n                await self.handle_cancellation(validation_result, session)\n            elif message_type == \"heartbeat\":\n                await self.handle_heartbeat(raw_message, session_id)\n            else:\n                # Handle other message types\n                await self.handle_standard_message(validation_result, session)\n                \n        except Exception as e:\n            # Create error context\n            context = ErrorContext(\n                error=e,\n                severity=self.determine_error_severity(e),\n                operation=\"process_message\",\n                session_id=session_id,\n                recoverable=True,\n                metadata={\"message_type\": raw_message.get(\"type\", \"unknown\")}\n            )\n            \n            # Handle the error\n            recovered = await handle_error(context)\n            if not recovered and session:\n                session.state = SessionState.ERROR\n    \n    async def handle_cancellation(self, validation_result: ValidationResult, session: Session) -> None:\n        \"\"\"Handle a cancellation notification.\"\"\"\n        if validation_result.is_valid and isinstance(validation_result.model, CancelledNotification):\n            # Get the properly validated model\n            notification = validation_result.model\n            request_id = notification.request_id\n            \n            logger.info(f\"Received cancellation for request {request_id} in session {session.session_id}\")\n            \n            # Remove from active operations\n            session.remove_operation(request_id)\n            \n            # Cancel any ongoing tasks for this request\n            if request_id in self.active_tasks:\n                task = self.active_tasks[request_id]\n                if not task.done():\n                    logger.info(f\"Cancelling task for request {request_id}\")\n                    task.cancel()\n                del self.active_tasks[request_id]\n        else:\n            # Use the raw data as fallback\n            raw_data = validation_result.raw_data\n            request_id = raw_data.get(\"request_id\", \"unknown\")\n            \n            logger.warning(f\"Processing invalid cancellation for request {request_id} with best effort\")\n            \n            # Best effort cancellation using raw data\n            if request_id != \"unknown\" and request_id in self.active_tasks:\n                task = self.active_tasks[request_id]\n                if not task.done():\n                    task.cancel()\n                del self.active_tasks[request_id]\n                session.remove_operation(request_id)\n    \n    def determine_error_severity(self, error: Exception) -> ErrorSeverity:\n        \"\"\"Determine the severity of an error.\"\"\"\n        # Classify errors by type\n        if isinstance(error, asyncio.CancelledError):\n            return ErrorSeverity.LOW  # Normal cancellation\n        \n        if isinstance(error, ValueError) or isinstance(error, TypeError):\n            return ErrorSeverity.MEDIUM  # Validation errors\n        \n        # Default to high for unknown errors\n        return ErrorSeverity.HIGH\n```",
      "testStrategy": "1. Test handling of valid cancellation notifications\n2. Test with malformed cancellation messages\n3. Verify active operations are properly cancelled\n4. Test error recovery during message processing\n5. Verify session state is maintained correctly\n6. Test with concurrent messages and cancellations",
      "priority": "high",
      "dependencies": [
        3,
        4,
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Enhance Logging and Monitoring",
      "description": "Implement comprehensive logging and monitoring to track session health, errors, and performance metrics.",
      "details": "1. Create a centralized logging configuration\n2. Add structured logging with contextual information\n3. Implement performance metrics collection\n4. Create health check endpoints\n5. Add detailed error logging with context\n\n```python\n# src/mcp_server_git/logging_config.py\nimport logging\nimport json\nimport time\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional\n\nclass StructuredLogFormatter(logging.Formatter):\n    \"\"\"Formatter that outputs JSON formatted logs.\"\"\"\n    \n    def format(self, record: logging.LogRecord) -> str:\n        log_data = {\n            \"timestamp\": datetime.fromtimestamp(record.created).isoformat(),\n            \"level\": record.levelname,\n            \"message\": record.getMessage(),\n            \"logger\": record.name,\n        }\n        \n        # Add extra fields from record\n        if hasattr(record, \"session_id\"):\n            log_data[\"session_id\"] = record.session_id\n        \n        if hasattr(record, \"request_id\"):\n            log_data[\"request_id\"] = record.request_id\n        \n        if hasattr(record, \"duration\"):\n            log_data[\"duration_ms\"] = record.duration\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_data[\"exception\"] = {\n                \"type\": record.exc_info[0].__name__,\n                \"message\": str(record.exc_info[1]),\n            }\n        \n        return json.dumps(log_data)\n\ndef configure_logging(log_level: str = \"INFO\") -> None:\n    \"\"\"Configure structured logging.\"\"\"\n    root_logger = logging.getLogger()\n    root_logger.setLevel(log_level)\n    \n    # Remove existing handlers\n    for handler in root_logger.handlers:\n        root_logger.removeHandler(handler)\n    \n    # Add console handler with structured formatter\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(StructuredLogFormatter())\n    root_logger.addHandler(console_handler)\n\n# src/mcp_server_git/metrics.py\nfrom typing import Dict, List, Any, Optional\nimport time\nimport asyncio\nfrom collections import defaultdict\n\nclass MetricsCollector:\n    \"\"\"Collects and reports metrics about server operation.\"\"\"\n    \n    def __init__(self):\n        self.metrics: Dict[str, Any] = {\n            \"sessions\": {\n                \"active\": 0,\n                \"total_created\": 0,\n                \"errors\": 0,\n            },\n            \"messages\": {\n                \"processed\": 0,\n                \"errors\": 0,\n                \"by_type\": defaultdict(int),\n            },\n            \"operations\": {\n                \"active\": 0,\n                \"completed\": 0,\n                \"cancelled\": 0,\n                \"failed\": 0,\n            },\n            \"performance\": {\n                \"message_processing_time\": [],  # Recent processing times in ms\n                \"avg_processing_time\": 0,\n            }\n        }\n        self.lock = asyncio.Lock()\n    \n    async def record_message(self, message_type: str, duration_ms: float) -> None:\n        \"\"\"Record a processed message.\"\"\"\n        async with self.lock:\n            self.metrics[\"messages\"][\"processed\"] += 1\n            self.metrics[\"messages\"][\"by_type\"][message_type] += 1\n            \n            # Track performance\n            self.metrics[\"performance\"][\"message_processing_time\"].append(duration_ms)\n            # Keep only recent measurements\n            if len(self.metrics[\"performance\"][\"message_processing_time\"]) > 100:\n                self.metrics[\"performance\"][\"message_processing_time\"].pop(0)\n            \n            # Update average\n            times = self.metrics[\"performance\"][\"message_processing_time\"]\n            self.metrics[\"performance\"][\"avg_processing_time\"] = sum(times) / len(times) if times else 0\n    \n    async def record_message_error(self, message_type: str) -> None:\n        \"\"\"Record a message processing error.\"\"\"\n        async with self.lock:\n            self.metrics[\"messages\"][\"errors\"] += 1\n    \n    async def record_session_created(self) -> None:\n        \"\"\"Record a new session creation.\"\"\"\n        async with self.lock:\n            self.metrics[\"sessions\"][\"active\"] += 1\n            self.metrics[\"sessions\"][\"total_created\"] += 1\n    \n    async def record_session_closed(self) -> None:\n        \"\"\"Record a session being closed.\"\"\"\n        async with self.lock:\n            self.metrics[\"sessions\"][\"active\"] -= 1\n    \n    async def record_operation_started(self) -> None:\n        \"\"\"Record an operation being started.\"\"\"\n        async with self.lock:\n            self.metrics[\"operations\"][\"active\"] += 1\n    \n    async def record_operation_completed(self, cancelled: bool = False, failed: bool = False) -> None:\n        \"\"\"Record an operation being completed.\"\"\"\n        async with self.lock:\n            self.metrics[\"operations\"][\"active\"] -= 1\n            \n            if cancelled:\n                self.metrics[\"operations\"][\"cancelled\"] += 1\n            elif failed:\n                self.metrics[\"operations\"][\"failed\"] += 1\n            else:\n                self.metrics[\"operations\"][\"completed\"] += 1\n    \n    async def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get a copy of the current metrics.\"\"\"\n        async with self.lock:\n            return dict(self.metrics)\n\n# Add performance tracking to server.py\nasync def process_message(self, raw_message: Dict[str, Any], session_id: str) -> None:\n    \"\"\"Process an incoming message with performance tracking.\"\"\"\n    start_time = time.time()\n    message_type = raw_message.get(\"type\", \"unknown\")\n    \n    try:\n        # ... existing message processing code ...\n        \n        # Record metrics\n        duration_ms = (time.time() - start_time) * 1000\n        await self.metrics.record_message(message_type, duration_ms)\n        \n        # Add performance info to logs\n        logger.info(f\"Processed {message_type} message\", extra={\n            \"session_id\": session_id,\n            \"duration\": duration_ms,\n            \"message_type\": message_type\n        })\n        \n    except Exception as e:\n        # Record error in metrics\n        await self.metrics.record_message_error(message_type)\n        \n        # ... existing error handling code ...\n```",
      "testStrategy": "1. Verify structured logs contain all required fields\n2. Test metrics collection for different operations\n3. Verify performance tracking accuracy\n4. Test concurrent metrics updates\n5. Verify error logging captures all necessary context\n6. Test log levels and filtering",
      "priority": "medium",
      "dependencies": [
        5,
        7
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Implement Graceful Shutdown and Recovery",
      "description": "Add mechanisms for graceful server shutdown and recovery from crashes to maintain session state.",
      "details": "1. Implement graceful shutdown sequence\n2. Add session state persistence\n3. Create recovery process for server restart\n4. Add signal handlers for controlled shutdown\n5. Implement cleanup of resources during shutdown\n\n```python\n# src/mcp_server_git/server.py\nimport asyncio\nimport signal\nimport json\nimport os\nfrom typing import Dict, Any, Optional, List, Set\n\nclass MCPGitServer:\n    # ... existing initialization code ...\n    \n    async def start(self) -> None:\n        \"\"\"Start the server with signal handling.\"\"\"\n        # Register signal handlers\n        for sig in (signal.SIGINT, signal.SIGTERM):\n            asyncio.get_event_loop().add_signal_handler(\n                sig, lambda: asyncio.create_task(self.shutdown()))\n        \n        # Start session manager and heartbeat\n        await self.session_manager.start()\n        await self.heartbeat_manager.start()\n        \n        # Restore any saved sessions\n        await self.restore_sessions()\n        \n        logger.info(\"MCP Git Server started successfully\")\n    \n    async def shutdown(self) -> None:\n        \"\"\"Gracefully shut down the server.\"\"\"\n        logger.info(\"Shutting down MCP Git Server...\")\n        \n        # Set shutting down flag\n        self.shutting_down = True\n        \n        # Save session state\n        await self.save_sessions()\n        \n        # Stop accepting new connections\n        # ... code to stop server ...\n        \n        # Give active operations time to complete\n        if self.active_tasks:\n            logger.info(f\"Waiting for {len(self.active_tasks)} active tasks to complete\")\n            try:\n                # Wait for tasks with timeout\n                pending_tasks = [task for task in self.active_tasks.values() if not task.done()]\n                await asyncio.wait(pending_tasks, timeout=5.0)\n            except asyncio.TimeoutError:\n                logger.warning(\"Some tasks did not complete in time\")\n        \n        # Cancel any remaining tasks\n        for task_id, task in self.active_tasks.items():\n            if not task.done():\n                logger.info(f\"Cancelling task {task_id}\")\n                task.cancel()\n        \n        # Close all sessions\n        sessions = await self.session_manager.get_all_sessions()\n        for session in sessions:\n            await self.session_manager.close_session(session.session_id)\n        \n        # Stop heartbeat manager\n        await self.heartbeat_manager.stop()\n        \n        # Stop session manager\n        await self.session_manager.stop()\n        \n        logger.info(\"MCP Git Server shutdown complete\")\n    \n    async def save_sessions(self) -> None:\n        \"\"\"Save session state to disk for recovery.\"\"\"\n        sessions = await self.session_manager.get_all_sessions()\n        if not sessions:\n            return\n        \n        session_data = [session.to_dict() for session in sessions]\n        \n        try:\n            os.makedirs(\"./data\", exist_ok=True)\n            with open(\"./data/sessions.json\", \"w\") as f:\n                json.dump(session_data, f)\n            logger.info(f\"Saved {len(sessions)} sessions to disk\")\n        except Exception as e:\n            logger.error(f\"Failed to save sessions: {e}\")\n    \n    async def restore_sessions(self) -> None:\n        \"\"\"Restore sessions from disk after restart.\"\"\"\n        try:\n            if not os.path.exists(\"./data/sessions.json\"):\n                return\n            \n            with open(\"./data/sessions.json\", \"r\") as f:\n                session_data = json.load(f)\n            \n            for data in session_data:\n                # Create new session with saved ID\n                session = await self.session_manager.create_session(data[\"session_id\"])\n                \n                # Restore session properties\n                session.state = SessionState(data[\"state\"])\n                session.created_at = data[\"created_at\"]\n                session.last_activity = data[\"last_activity\"]\n                session.error_count = data[\"error_count\"]\n                session.message_count = data[\"message_count\"]\n                session.client_info = data[\"client_info\"]\n                session.metadata = data[\"metadata\"]\n                session.active_operations = set(data[\"active_operations\"])\n            \n            logger.info(f\"Restored {len(session_data)} sessions from disk\")\n            \n            # Clean up the file after successful restore\n            os.remove(\"./data/sessions.json\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to restore sessions: {e}\")\n```",
      "testStrategy": "1. Test graceful shutdown with active operations\n2. Verify session state is saved correctly\n3. Test session restoration after restart\n4. Verify signal handlers work as expected\n5. Test resource cleanup during shutdown\n6. Verify no resource leaks after shutdown",
      "priority": "medium",
      "dependencies": [
        7,
        9
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Implement Comprehensive Integration Tests",
      "description": "Create a suite of integration tests to verify protocol compliance and stability under various conditions.",
      "details": "1. Create test fixtures for server and client setup\n2. Implement tests for all notification types\n3. Add tests for error scenarios and recovery\n4. Create long-running session tests\n5. Implement stress tests with high message volume\n\n```python\n# src/tests/integration/test_protocol_compliance.py\nimport pytest\nimport asyncio\nimport uuid\nfrom typing import Dict, Any, List\n\nfrom mcp_server_git.server import MCPGitServer\nfrom mcp_client import MCPClient  # Mock client for testing\n\n@pytest.fixture\nasync def server():\n    \"\"\"Set up a test server instance.\"\"\"\n    server = MCPGitServer()\n    await server.start()\n    yield server\n    await server.shutdown()\n\n@pytest.fixture\nasync def client(server):\n    \"\"\"Set up a test client connected to the server.\"\"\"\n    client = MCPClient()\n    await client.connect()\n    yield client\n    await client.disconnect()\n\n@pytest.mark.asyncio\nasync def test_cancelled_notification(server, client):\n    \"\"\"Test handling of cancelled notifications.\"\"\"\n    # Start a long-running operation\n    operation_id = str(uuid.uuid4())\n    await client.start_operation(operation_id)\n    \n    # Send cancellation\n    cancel_notification = {\n        \"type\": \"notifications/cancelled\",\n        \"id\": str(uuid.uuid4()),\n        \"request_id\": operation_id,\n        \"reason\": \"Test cancellation\"\n    }\n    await client.send_notification(cancel_notification)\n    \n    # Verify operation was cancelled\n    await asyncio.sleep(0.1)  # Give server time to process\n    assert operation_id not in server.active_tasks\n    \n    # Verify server is still responsive\n    response = await client.ping()\n    assert response[\"type\"] == \"pong\"\n\n@pytest.mark.asyncio\nasync def test_malformed_notification(server, client):\n    \"\"\"Test handling of malformed notifications.\"\"\"\n    # Send malformed notification\n    bad_notification = {\n        \"type\": \"notifications/cancelled\",\n        # Missing required fields\n        \"id\": str(uuid.uuid4())\n    }\n    await client.send_notification(bad_notification)\n    \n    # Verify server is still responsive\n    response = await client.ping()\n    assert response[\"type\"] == \"pong\"\n\n@pytest.mark.asyncio\nasync def test_unknown_notification_type(server, client):\n    \"\"\"Test handling of unknown notification types.\"\"\"\n    # Send unknown notification type\n    unknown_notification = {\n        \"type\": \"notifications/unknown_type\",\n        \"id\": str(uuid.uuid4()),\n        \"data\": {\"foo\": \"bar\"}\n    }\n    await client.send_notification(unknown_notification)\n    \n    # Verify server is still responsive\n    response = await client.ping()\n    assert response[\"type\"] == \"pong\"\n\n# src/tests/integration/test_stability.py\n@pytest.mark.asyncio\nasync def test_long_running_session(server):\n    \"\"\"Test stability of a long-running session.\"\"\"\n    client = MCPClient()\n    await client.connect()\n    \n    # Run for 5 minutes in test (would be longer in CI)\n    end_time = asyncio.get_event_loop().time() + 300\n    message_count = 0\n    \n    try:\n        while asyncio.get_event_loop().time() < end_time:\n            # Mix of different operations\n            if message_count % 5 == 0:\n                # Start and cancel an operation\n                op_id = str(uuid.uuid4())\n                await client.start_operation(op_id)\n                await client.cancel_operation(op_id)\n            else:\n                # Regular ping\n                await client.ping()\n            \n            message_count += 1\n            await asyncio.sleep(0.1)\n    finally:\n        await client.disconnect()\n    \n    # Verify server is still healthy\n    metrics = await server.metrics.get_metrics()\n    assert metrics[\"sessions\"][\"active\"] == 0  # Client disconnected\n    assert metrics[\"messages\"][\"processed\"] >= message_count\n    assert metrics[\"messages\"][\"errors\"] == 0\n\n@pytest.mark.asyncio\nasync def test_concurrent_clients(server):\n    \"\"\"Test server with multiple concurrent clients.\"\"\"\n    clients = []\n    client_count = 10\n    \n    # Connect multiple clients\n    for i in range(client_count):\n        client = MCPClient()\n        await client.connect()\n        clients.append(client)\n    \n    # Have all clients send messages concurrently\n    async def client_task(client, count):\n        for i in range(count):\n            await client.ping()\n            if i % 3 == 0:\n                # Mix in some cancellations\n                op_id = str(uuid.uuid4())\n                await client.start_operation(op_id)\n                await client.cancel_operation(op_id)\n    \n    tasks = [client_task(client, 50) for client in clients]\n    await asyncio.gather(*tasks)\n    \n    # Disconnect all clients\n    for client in clients:\n        await client.disconnect()\n    \n    # Verify server handled all clients\n    metrics = await server.metrics.get_metrics()\n    assert metrics[\"sessions\"][\"total_created\"] >= client_count\n    assert metrics[\"sessions\"][\"active\"] == 0  # All disconnected\n```",
      "testStrategy": "1. Run tests in CI environment\n2. Verify all notification types are handled correctly\n3. Test with malformed and unexpected messages\n4. Run long-running tests (48+ hours in CI)\n5. Test with high message volume and concurrent clients\n6. Verify metrics and logs for test scenarios",
      "priority": "high",
      "dependencies": [
        2,
        3,
        4,
        9
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Performance Benchmarks and Optimization",
      "description": "Create performance benchmarks and optimize critical code paths to ensure the server meets performance requirements.",
      "details": "1. Create performance benchmark suite\n2. Identify and optimize critical code paths\n3. Implement memory usage tracking\n4. Add CPU profiling for key operations\n5. Optimize message validation and processing\n\n```python\n# src/tests/benchmarks/test_performance.py\nimport pytest\nimport asyncio\nimport time\nimport uuid\nimport random\nfrom typing import Dict, Any, List\n\nfrom mcp_server_git.server import MCPGitServer\nfrom mcp_client import MCPClient  # Mock client for testing\n\n@pytest.fixture\nasync def server():\n    \"\"\"Set up a test server instance.\"\"\"\n    server = MCPGitServer()\n    await server.start()\n    yield server\n    await server.shutdown()\n\n@pytest.mark.benchmark\nasync def test_message_processing_throughput(server, benchmark):\n    \"\"\"Benchmark message processing throughput.\"\"\"\n    client = MCPClient()\n    await client.connect()\n    \n    # Prepare a batch of messages\n    message_count = 1000\n    messages = []\n    \n    for i in range(message_count):\n        if i % 5 == 0:\n            # Mix in some cancellations\n            messages.append({\n                \"type\": \"notifications/cancelled\",\n                \"id\": str(uuid.uuid4()),\n                \"request_id\": str(uuid.uuid4()),\n                \"reason\": \"Benchmark cancellation\"\n            })\n        else:\n            # Regular ping messages\n            messages.append({\n                \"type\": \"ping\",\n                \"id\": str(uuid.uuid4())\n            })\n    \n    # Benchmark function\n    async def send_messages():\n        for msg in messages:\n            await client.send_message(msg)\n            # Small delay to avoid overwhelming\n            await asyncio.sleep(0.001)\n    \n    # Run benchmark\n    start_time = time.time()\n    await send_messages()\n    end_time = time.time()\n    \n    # Calculate throughput\n    duration = end_time - start_time\n    throughput = message_count / duration\n    \n    # Cleanup\n    await client.disconnect()\n    \n    # Report results\n    print(f\"Processed {message_count} messages in {duration:.2f}s\")\n    print(f\"Throughput: {throughput:.2f} messages/second\")\n    \n    # Verify performance meets requirements\n    assert throughput >= 100, \"Throughput below 100 messages/second\"\n\n@pytest.mark.benchmark\nasync def test_memory_usage(server):\n    \"\"\"Test memory usage under load.\"\"\"\n    import psutil\n    import os\n    \n    process = psutil.Process(os.getpid())\n    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n    \n    # Connect multiple clients\n    clients = []\n    for i in range(10):\n        client = MCPClient()\n        await client.connect()\n        clients.append(client)\n    \n    # Generate load\n    message_count = 1000\n    for i in range(message_count):\n        client = random.choice(clients)\n        await client.ping()\n    \n    # Check memory after load\n    final_memory = process.memory_info().rss / 1024 / 1024  # MB\n    memory_increase = final_memory - initial_memory\n    \n    # Cleanup\n    for client in clients:\n        await client.disconnect()\n    \n    print(f\"Initial memory: {initial_memory:.2f} MB\")\n    print(f\"Final memory: {final_memory:.2f} MB\")\n    print(f\"Memory increase: {memory_increase:.2f} MB\")\n    \n    # Verify memory usage meets requirements\n    assert final_memory < 100, \"Memory usage exceeds 100MB\"\n    assert memory_increase < 50, \"Memory increase exceeds 50MB during test\"\n\n# src/mcp_server_git/optimizations.py\nfrom typing import Dict, Any, Type, TypeVar, Optional\nfrom pydantic import BaseModel\nimport functools\n\nT = TypeVar('T', bound=BaseModel)\n\n# Cache for model validation\n_model_cache: Dict[str, Any] = {}\n_cache_size = 100\n\ndef get_cached_model(data: Dict[str, Any], model_class: Type[T]) -> Optional[T]:\n    \"\"\"Get a cached model instance if the data matches a previous validation.\"\"\"\n    # Create a cache key based on model class and relevant data fields\n    # This is a simplified example - real implementation would need a better cache key\n    model_name = model_class.__name__\n    \n    # For notifications, type and id are usually sufficient to identify the structure\n    if \"type\" in data and \"id\" in data:\n        cache_key = f\"{model_name}:{data['type']}:{hash(frozenset(data.keys()))}\"\n        \n        if cache_key in _model_cache:\n            cached_model, field_validators = _model_cache[cache_key]\n            \n            # Check if all validators would pass\n            try:\n                for field, validator in field_validators.items():\n                    if field in data:\n                        validator(data[field])\n                return cached_model.copy(update=data)\n            except Exception:\n                # Validation failed, need to create a new model\n                pass\n    \n    return None\n\ndef cache_model(model: BaseModel, data: Dict[str, Any]) -> None:\n    \"\"\"Cache a successfully validated model.\"\"\"\n    model_name = model.__class__.__name__\n    \n    if \"type\" in data and \"id\" in data:\n        cache_key = f\"{model_name}:{data['type']}:{hash(frozenset(data.keys()))}\"\n        \n        # Extract field validators for future checks\n        field_validators = {}\n        for field_name, field in model.__fields__.items():\n            if field.validators:\n                field_validators[field_name] = functools.partial(field.validate, field_name)\n        \n        _model_cache[cache_key] = (model, field_validators)\n        \n        # Limit cache size\n        if len(_model_cache) > _cache_size:\n            # Remove oldest entry\n            oldest_key = next(iter(_model_cache))\n            del _model_cache[oldest_key]\n\n# Optimized validation function\ndef optimized_validate_message(data: Dict[str, Any], model_class: Type[T]) -> ValidationResult[T]:\n    \"\"\"Optimized message validation with caching.\"\"\"\n    # Try to get from cache first\n    cached_model = get_cached_model(data, model_class)\n    if cached_model:\n        return ValidationResult(model=cached_model, raw_data=data)\n    \n    # Not in cache, validate normally\n    try:\n        model = model_class.parse_obj(data)\n        # Cache the successful validation\n        cache_model(model, data)\n        return ValidationResult(model=model, raw_data=data)\n    except ValidationError as e:\n        logger.warning(f\"Validation error for {model_class.__name__}: {e}\")\n        return ValidationResult(error=e, raw_data=data)\n```",
      "testStrategy": "1. Run benchmarks to measure message throughput\n2. Test memory usage under load\n3. Profile CPU usage during high-volume operations\n4. Compare performance before and after optimizations\n5. Verify performance meets requirements\n6. Test with realistic workloads",
      "priority": "medium",
      "dependencies": [
        9,
        10
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Create Technical Documentation",
      "description": "Develop comprehensive technical documentation for the enhanced protocol compliance and error handling systems.",
      "details": "1. Document protocol compliance implementation\n2. Create architecture diagrams\n3. Document error handling strategies\n4. Create API documentation\n5. Add code examples and usage patterns\n\n```markdown\n# MCP Server Protocol Compliance & Stability Documentation\n\n## Architecture Overview\n\nThe MCP Git Server implements a layered architecture for handling MCP protocol messages:\n\n1. **Message Validation Layer**: Validates incoming messages against Pydantic models\n2. **Message Routing Layer**: Routes messages to appropriate handlers based on type\n3. **Operation Execution Layer**: Executes Git operations based on messages\n4. **Error Handling Layer**: Provides recovery mechanisms for errors\n5. **Session Management Layer**: Maintains session state and health\n\n### Architecture Diagram\n\n```\nMCP Client → Protocol Validation → Message Routing → Git Operations → Response\n              ↓ (on error)           ↓ (on error)      ↓ (on error)\n           Error Recovery ← Error Logging ← Session Management\n```\n\n## Protocol Compliance\n\n### Notification Types\n\nThe server supports the following MCP notification types:\n\n- `notifications/cancelled`: Cancellation of an operation\n- `notifications/progress`: Progress updates for long-running operations\n- `notifications/log`: Log messages from operations\n- `notifications/status`: Status updates for operations\n\n### Message Validation\n\nAll messages are validated using Pydantic models with the following features:\n\n- Strict validation for required fields\n- Optional fields with sensible defaults\n- Flexible validation for unknown fields\n- Graceful handling of validation errors\n\n### Cancellation Handling\n\nCancellation notifications are handled as follows:\n\n1. Validate the cancellation notification\n2. Identify the operation being cancelled\n3. Cancel any active tasks for the operation\n4. Remove the operation from active operations list\n5. Send acknowledgement to the client\n\n## Error Handling\n\n### Error Classification\n\nErrors are classified into the following categories:\n\n- **Critical**: Require session termination\n- **High**: Operation must abort, but session can continue\n- **Medium**: Operation might recover\n- **Low**: Can safely ignore\n\n### Recovery Strategies\n\nThe following recovery strategies are implemented:\n\n- **Retry**: Automatically retry transient errors\n- **Circuit Breaker**: Prevent cascading failures\n- **Fallback**: Use default values when validation fails\n- **Graceful Degradation**: Continue with reduced functionality\n\n### Circuit Breaker Pattern\n\nThe circuit breaker pattern is implemented with three states:\n\n1. **Closed**: Normal operation, allowing requests\n2. **Open**: Failing fast, not allowing requests\n3. **Half-Open**: Testing if system has recovered\n\n## Session Management\n\n### Session Lifecycle\n\nSessions go through the following states:\n\n1. **Initializing**: Session is being created\n2. **Active**: Session is actively processing messages\n3. **Paused**: Session is temporarily paused\n4. **Error**: Session encountered an error\n5. **Closing**: Session is being closed\n6. **Closed**: Session is closed\n\n### Heartbeat Mechanism\n\nThe heartbeat mechanism works as follows:\n\n1. Server sends heartbeat requests at configurable intervals\n2. Client responds with heartbeat responses\n3. Server tracks missed heartbeats\n4. After threshold of missed heartbeats, session is considered disconnected\n\n## Performance Considerations\n\n### Message Processing Optimization\n\n- Validation caching for similar messages\n- Optimized JSON parsing\n- Asynchronous processing for long-running operations\n- Memory usage optimization\n\n### Benchmarks\n\nThe server meets the following performance targets:\n\n- **Message Processing**: <100ms per message\n- **Memory Usage**: <50MB base footprint\n- **CPU Usage**: <5% during idle periods\n- **Throughput**: >100 messages per second\n\n## API Reference\n\n### Server Configuration\n\n```python\nserver = MCPGitServer(\n    heartbeat_interval=30.0,  # Seconds between heartbeats\n    missed_heartbeat_threshold=3,  # Number of missed heartbeats before disconnect\n    max_error_count=10,  # Maximum errors before session termination\n    recovery_timeout=30.0,  # Seconds before retry after circuit trip\n)\n```\n\n### Message Handling\n\n```python\n# Register a custom message handler\n@server.message_handler(\"custom/message\")\nasync def handle_custom_message(message, session):\n    # Handle custom message\n    pass\n```\n\n### Error Handling\n\n```python\n# Use recoverable decorator for functions that should retry on failure\n@recoverable(max_retries=3, backoff_factor=1.0)\nasync def some_operation():\n    # Operation that might fail\n    pass\n\n# Use circuit breaker for operations that might cause cascading failures\ncircuit = CircuitBreaker(\"git_operations\", failure_threshold=5)\n\n@with_circuit_breaker(circuit)\nasync def git_operation():\n    # Git operation that might fail\n    pass\n```\n\n## Troubleshooting\n\n### Common Errors\n\n- **Validation Errors**: Check message format against MCP specification\n- **Circuit Open**: Too many failures, wait for recovery timeout\n- **Session Closed**: Session was closed due to inactivity or errors\n- **Operation Cancelled**: Operation was cancelled by client\n\n### Logging\n\nLogs are structured in JSON format with the following fields:\n\n- `timestamp`: ISO 8601 timestamp\n- `level`: Log level (INFO, WARNING, ERROR, etc.)\n- `message`: Log message\n- `session_id`: Session identifier\n- `request_id`: Request identifier\n- `duration_ms`: Operation duration in milliseconds\n- `exception`: Exception details if applicable\n\n### Metrics\n\nThe following metrics are available:\n\n- **Sessions**: Active, total created, errors\n- **Messages**: Processed, errors, by type\n- **Operations**: Active, completed, cancelled, failed\n- **Performance**: Message processing time, average processing time\n```",
      "testStrategy": "1. Review documentation for accuracy and completeness\n2. Verify code examples match actual implementation\n3. Test documentation with new team members for clarity\n4. Ensure all error handling strategies are documented\n5. Verify API reference is complete\n6. Check troubleshooting guide covers common issues",
      "priority": "medium",
      "dependencies": [
        2,
        3,
        4,
        5,
        6,
        7,
        9
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Implement Stress Testing and Validation",
      "description": "Create comprehensive stress tests to validate server stability under extreme conditions and for extended periods.",
      "details": "1. Implement long-running stress test suite\n2. Create tests for high message volume\n3. Add tests for error injection\n4. Implement resource leak detection\n5. Create tests for concurrent client connections\n\n```python\n# src/tests/stress/test_long_running.py\nimport pytest\nimport asyncio\nimport uuid\nimport random\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any, List\n\nfrom mcp_server_git.server import MCPGitServer\nfrom mcp_client import MCPClient  # Mock client for testing\n\n@pytest.mark.stress\nasync def test_48_hour_stability(server):\n    \"\"\"Test server stability over a 48-hour period.\"\"\"\n    # For CI, we can scale this down but log more frequently\n    test_duration_hours = 48  # Actual test duration\n    scaled_duration_minutes = 30  # Scaled duration for CI\n    \n    # Scale factor for accelerated testing\n    scale_factor = (test_duration_hours * 60) / scaled_duration_minutes\n    \n    # Connect a client\n    client = MCPClient()\n    await client.connect()\n    \n    # Track metrics\n    start_time = time.time()\n    end_time = start_time + (scaled_duration_minutes * 60)\n    message_count = 0\n    error_count = 0\n    operation_count = 0\n    cancel_count = 0\n    \n    # Log initial state\n    print(f\"Starting {test_duration_hours}-hour stability test (scaled to {scaled_duration_minutes} minutes)\")\n    print(f\"Start time: {datetime.now().isoformat()}\")\n    \n    try:\n        while time.time() < end_time:\n            # Simulate a mix of operations\n            operation_type = random.randint(1, 10)\n            \n            if operation_type <= 3:\n                # Start a long-running operation\n                op_id = str(uuid.uuid4())\n                await client.start_operation(op_id)\n                operation_count += 1\n                \n                # 50% chance to cancel it\n                if random.random() < 0.5:\n                    await asyncio.sleep(random.random() * 0.5)  # Wait a bit\n                    await client.cancel_operation(op_id)\n                    cancel_count += 1\n            \n            elif operation_type <= 6:\n                # Send a batch of messages\n                batch_size = random.randint(5, 20)\n                for _ in range(batch_size):\n                    await client.ping()\n                    message_count += 1\n            \n            elif operation_type <= 8:\n                # Simulate a client reconnection\n                await client.disconnect()\n                await asyncio.sleep(0.5)\n                await client.connect()\n            \n            elif operation_type <= 9:\n                # Send an invalid message to test error handling\n                try:\n                    await client.send_invalid_message()\n                    error_count += 1\n                except Exception:\n                    pass\n            \n            else:\n                # Just wait a bit to simulate idle time\n                await asyncio.sleep(random.random() * 2.0)\n            \n            # Log progress periodically\n            elapsed = time.time() - start_time\n            if int(elapsed) % 60 == 0:  # Log every minute\n                simulated_hours = (elapsed / 60) * scale_factor / 60\n                print(f\"Progress: {simulated_hours:.2f} simulated hours\")\n                print(f\"Messages: {message_count}, Operations: {operation_count}, Cancellations: {cancel_count}, Errors: {error_count}\")\n                \n                # Get server metrics\n                metrics = await server.metrics.get_metrics()\n                print(f\"Server metrics: {metrics}\")\n            \n            # Small delay between operations\n            await asyncio.sleep(0.05)\n    \n    finally:\n        # Disconnect client\n        await client.disconnect()\n    \n    # Log final state\n    print(f\"Test completed at {datetime.now().isoformat()}\")\n    print(f\"Total messages: {message_count}\")\n    print(f\"Total operations: {operation_count}\")\n    print(f\"Total cancellations: {cancel_count}\")\n    print(f\"Total errors: {error_count}\")\n    \n    # Get final server metrics\n    metrics = await server.metrics.get_metrics()\n    print(f\"Final server metrics: {metrics}\")\n    \n    # Verify server is still healthy\n    assert metrics[\"sessions\"][\"active\"] == 0  # All sessions closed\n    assert metrics[\"operations\"][\"active\"] == 0  # No lingering operations\n    \n    # Check error rate is within acceptable limits\n    total_messages = metrics[\"messages\"][\"processed\"]\n    error_rate = metrics[\"messages\"][\"errors\"] / total_messages if total_messages > 0 else 0\n    assert error_rate < 0.01, f\"Error rate too high: {error_rate:.2%}\"\n\n# src/tests/stress/test_error_injection.py\n@pytest.mark.stress\nasync def test_error_injection(server):\n    \"\"\"Test server stability with deliberate error injection.\"\"\"\n    client = MCPClient()\n    await client.connect()\n    \n    # Define error scenarios to inject\n    error_scenarios = [\n        # Malformed messages\n        {\"type\": \"malformed\", \"id\": str(uuid.uuid4()), \"missing_required_fields\": True},\n        \n        # Invalid field types\n        {\"type\": \"notifications/cancelled\", \"id\": 12345, \"request_id\": 67890},  # IDs should be strings\n        \n        # Unknown message types\n        {\"type\": \"unknown/message_type\", \"id\": str(uuid.uuid4())},\n        \n        # Empty message\n        {},\n        \n        # Oversized message\n        {\"type\": \"oversized\", \"id\": str(uuid.uuid4()), \"data\": \"x\" * 1000000},\n        \n        # Nested invalid structures\n        {\"type\": \"nested_invalid\", \"id\": str(uuid.uuid4()), \"data\": {\"nested\": {\"invalid\": [1, 2, None, {\"x\": float('nan')}]}}},\n    ]\n    \n    # Send each error scenario\n    for scenario in error_scenarios:\n        print(f\"Injecting error scenario: {scenario['type'] if 'type' in scenario else 'unknown'}\")\n        \n        try:\n            await client.send_raw_message(scenario)\n        except Exception as e:\n            print(f\"Client exception (expected): {e}\")\n        \n        # Verify server is still responsive after each error\n        try:\n            response = await client.ping()\n            assert response[\"type\"] == \"pong\", \"Server failed to respond to ping\"\n            print(\"Server still responsive ✓\")\n        except Exception as e:\n            pytest.fail(f\"Server became unresponsive after error injection: {e}\")\n    \n    # Disconnect client\n    await client.disconnect()\n    \n    # Verify server metrics\n    metrics = await server.metrics.get_metrics()\n    print(f\"Final metrics after error injection: {metrics}\")\n    \n    # Server should have recorded the errors but remained stable\n    assert metrics[\"messages\"][\"errors\"] >= len(error_scenarios), \"Not all errors were recorded\"\n    assert metrics[\"sessions\"][\"active\"] == 0, \"Sessions were not properly closed\"\n\n# src/tests/stress/test_resource_leaks.py\n@pytest.mark.stress\nasync def test_memory_leak_detection(server):\n    \"\"\"Test for memory leaks during extended operation.\"\"\"\n    import psutil\n    import os\n    import gc\n    \n    process = psutil.Process(os.getpid())\n    \n    # Force garbage collection to get accurate baseline\n    gc.collect()\n    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n    \n    client = MCPClient()\n    await client.connect()\n    \n    # Run a large number of operations\n    operation_count = 10000\n    memory_samples = []\n    \n    for i in range(operation_count):\n        # Mix of different operations\n        if i % 100 == 0:\n            # Log progress and take memory sample\n            current_memory = process.memory_info().rss / 1024 / 1024  # MB\n            memory_samples.append(current_memory)\n            print(f\"Operation {i}/{operation_count}, Memory: {current_memory:.2f} MB\")\n        \n        if i % 10 == 0:\n            # Start and cancel an operation\n            op_id = str(uuid.uuid4())\n            await client.start_operation(op_id)\n            await client.cancel_operation(op_id)\n        else:\n            # Regular ping\n            await client.ping()\n    \n    # Disconnect client\n    await client.disconnect()\n    \n    # Force garbage collection again\n    gc.collect()\n    final_memory = process.memory_info().rss / 1024 / 1024  # MB\n    \n    # Calculate memory growth\n    memory_diff = final_memory - initial_memory\n    print(f\"Initial memory: {initial_memory:.2f} MB\")\n    print(f\"Final memory: {final_memory:.2f} MB\")\n    print(f\"Memory difference: {memory_diff:.2f} MB\")\n    \n    # Check for memory growth trend in samples\n    if len(memory_samples) > 10:\n        # Calculate slope of memory growth\n        x = list(range(len(memory_samples)))\n        y = memory_samples\n        n = len(x)\n        \n        # Simple linear regression to detect trend\n        slope = (n * sum(x[i] * y[i] for i in range(n)) - sum(x) * sum(y)) / \\\n                (n * sum(x[i]**2 for i in range(n)) - sum(x)**2)\n        \n        print(f\"Memory growth slope: {slope:.6f} MB/sample\")\n        \n        # A significant positive slope indicates a leak\n        assert slope < 0.01, f\"Detected potential memory leak: {slope:.6f} MB/sample\"\n    \n    # Check absolute growth\n    assert memory_diff < 10, f\"Memory increased by {memory_diff:.2f} MB, possible leak\"\n```",
      "testStrategy": "1. Run long-running tests in CI environment (48+ hours)\n2. Monitor memory usage for leaks\n3. Test with high message volume (10,000+ messages)\n4. Inject various error scenarios\n5. Test with multiple concurrent clients\n6. Verify metrics and logs during stress tests",
      "priority": "high",
      "dependencies": [
        9,
        10,
        11,
        12
      ],
      "status": "pending",
      "subtasks": []
    }
  ]
}