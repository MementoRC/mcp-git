# Task ID: 13
# Title: Implement Performance Benchmarks and Optimization
# Status: pending
# Dependencies: 9, 10
# Priority: medium
# Description: Create performance benchmarks and optimize critical code paths to ensure the server meets performance requirements.
# Details:
1. Create performance benchmark suite
2. Identify and optimize critical code paths
3. Implement memory usage tracking
4. Add CPU profiling for key operations
5. Optimize message validation and processing

```python
# src/tests/benchmarks/test_performance.py
import pytest
import asyncio
import time
import uuid
import random
from typing import Dict, Any, List

from mcp_server_git.server import MCPGitServer
from mcp_client import MCPClient  # Mock client for testing

@pytest.fixture
async def server():
    """Set up a test server instance."""
    server = MCPGitServer()
    await server.start()
    yield server
    await server.shutdown()

@pytest.mark.benchmark
async def test_message_processing_throughput(server, benchmark):
    """Benchmark message processing throughput."""
    client = MCPClient()
    await client.connect()

    # Prepare a batch of messages
    message_count = 1000
    messages = []

    for i in range(message_count):
        if i % 5 == 0:
            # Mix in some cancellations
            messages.append({
                "type": "notifications/cancelled",
                "id": str(uuid.uuid4()),
                "request_id": str(uuid.uuid4()),
                "reason": "Benchmark cancellation"
            })
        else:
            # Regular ping messages
            messages.append({
                "type": "ping",
                "id": str(uuid.uuid4())
            })

    # Benchmark function
    async def send_messages():
        for msg in messages:
            await client.send_message(msg)
            # Small delay to avoid overwhelming
            await asyncio.sleep(0.001)

    # Run benchmark
    start_time = time.time()
    await send_messages()
    end_time = time.time()

    # Calculate throughput
    duration = end_time - start_time
    throughput = message_count / duration

    # Cleanup
    await client.disconnect()

    # Report results
    print(f"Processed {message_count} messages in {duration:.2f}s")
    print(f"Throughput: {throughput:.2f} messages/second")

    # Verify performance meets requirements
    assert throughput >= 100, "Throughput below 100 messages/second"

@pytest.mark.benchmark
async def test_memory_usage(server):
    """Test memory usage under load."""
    import psutil
    import os

    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB

    # Connect multiple clients
    clients = []
    for i in range(10):
        client = MCPClient()
        await client.connect()
        clients.append(client)

    # Generate load
    message_count = 1000
    for i in range(message_count):
        client = random.choice(clients)
        await client.ping()

    # Check memory after load
    final_memory = process.memory_info().rss / 1024 / 1024  # MB
    memory_increase = final_memory - initial_memory

    # Cleanup
    for client in clients:
        await client.disconnect()

    print(f"Initial memory: {initial_memory:.2f} MB")
    print(f"Final memory: {final_memory:.2f} MB")
    print(f"Memory increase: {memory_increase:.2f} MB")

    # Verify memory usage meets requirements
    assert final_memory < 100, "Memory usage exceeds 100MB"
    assert memory_increase < 50, "Memory increase exceeds 50MB during test"

# src/mcp_server_git/optimizations.py
from typing import Dict, Any, Type, TypeVar, Optional
from pydantic import BaseModel
import functools

T = TypeVar('T', bound=BaseModel)

# Cache for model validation
_model_cache: Dict[str, Any] = {}
_cache_size = 100

def get_cached_model(data: Dict[str, Any], model_class: Type[T]) -> Optional[T]:
    """Get a cached model instance if the data matches a previous validation."""
    # Create a cache key based on model class and relevant data fields
    # This is a simplified example - real implementation would need a better cache key
    model_name = model_class.__name__

    # For notifications, type and id are usually sufficient to identify the structure
    if "type" in data and "id" in data:
        cache_key = f"{model_name}:{data['type']}:{hash(frozenset(data.keys()))}"

        if cache_key in _model_cache:
            cached_model, field_validators = _model_cache[cache_key]

            # Check if all validators would pass
            try:
                for field, validator in field_validators.items():
                    if field in data:
                        validator(data[field])
                return cached_model.copy(update=data)
            except Exception:
                # Validation failed, need to create a new model
                pass

    return None

def cache_model(model: BaseModel, data: Dict[str, Any]) -> None:
    """Cache a successfully validated model."""
    model_name = model.__class__.__name__

    if "type" in data and "id" in data:
        cache_key = f"{model_name}:{data['type']}:{hash(frozenset(data.keys()))}"

        # Extract field validators for future checks
        field_validators = {}
        for field_name, field in model.__fields__.items():
            if field.validators:
                field_validators[field_name] = functools.partial(field.validate, field_name)

        _model_cache[cache_key] = (model, field_validators)

        # Limit cache size
        if len(_model_cache) > _cache_size:
            # Remove oldest entry
            oldest_key = next(iter(_model_cache))
            del _model_cache[oldest_key]

# Optimized validation function
def optimized_validate_message(data: Dict[str, Any], model_class: Type[T]) -> ValidationResult[T]:
    """Optimized message validation with caching."""
    # Try to get from cache first
    cached_model = get_cached_model(data, model_class)
    if cached_model:
        return ValidationResult(model=cached_model, raw_data=data)

    # Not in cache, validate normally
    try:
        model = model_class.parse_obj(data)
        # Cache the successful validation
        cache_model(model, data)
        return ValidationResult(model=model, raw_data=data)
    except ValidationError as e:
        logger.warning(f"Validation error for {model_class.__name__}: {e}")
        return ValidationResult(error=e, raw_data=data)
```

# Test Strategy:
1. Run benchmarks to measure message throughput
2. Test memory usage under load
3. Profile CPU usage during high-volume operations
4. Compare performance before and after optimizations
5. Verify performance meets requirements
6. Test with realistic workloads
